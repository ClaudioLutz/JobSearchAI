# Story 2.5: Migration & Production Deployment

## Status
**Draft**

---

## Story

**As a** system administrator,
**I want** to migrate existing JSON data to the database and deploy the new system to production,
**so that** users can benefit from the deduplication and performance improvements.

---

## Acceptance Criteria

1. Data migration script created (`scripts/migrate_json_to_sqlite.py`) that:
   - Discovers all existing JSON match files in `job_matches/` directory
   - For each file, prompts user for metadata (search_term, cv_key)
   - Migrates match data to database with proper structure
   - Handles duplicates gracefully (skip or update)
   - Logs migration progress and statistics
   - Creates backup before migration

2. Migration validation script created:
   - Verifies all JSON data migrated successfully
   - Counts records: JSON vs database
   - Identifies any missing or corrupted data
   - Generates migration report

3. End-to-end testing suite created:
   - Test complete scrape → match → display workflow
   - Test duplicate detection across all components
   - Test early exit optimization
   - Test multi-search scenario
   - Test CV version change scenario
   - Verify all integration points work

4. Performance benchmarks documented:
   - Measure API call reduction (target: 90%)
   - Measure page scraping reduction (target: 70%)
   - Measure query response time (target: <100ms)
   - Compare before/after metrics
   - Document cost savings

5. Production deployment checklist completed:
   - Database backup created
   - Code deployed to production
   - Database schema initialized
   - Migration script executed
   - Smoke tests passed
   - Monitoring configured
   - Rollback plan documented

6. Documentation updated:
   - User guide updated with new features
   - Development guide updated with database info
   - Troubleshooting guide created
   - API changes documented

7. Monitoring and alerting configured:
   - Database performance monitoring
   - API usage tracking
   - Error rate monitoring
   - Deduplication effectiveness tracking

8. Production verification completed:
   - Zero critical bugs in first 48 hours
   - Performance targets met
   - No data loss
   - No regression in existing functionality
   - User acceptance testing passed

---

## Tasks / Subtasks

- [ ] Create data migration script (AC: 1)
  - [ ] Create `scripts/migrate_json_to_sqlite.py`
  - [ ] Add JSON file discovery (glob for `job_matches/job_matches_*.json`)
  - [ ] For each file, prompt user for search_term and cv_path
  - [ ] Generate or prompt for cv_key
  - [ ] Read JSON match data
  - [ ] Transform to database format
  - [ ] Insert into database with error handling (IntegrityError = skip)
  - [ ] Track statistics: migrated, duplicates, errors
  - [ ] Display progress bar or status updates
  - [ ] Generate migration report at end
  - [ ] Add `--dry-run` option to preview migration
  - [ ] Add `--force` option to overwrite duplicates

- [ ] Create database backup utility (AC: 1, 5)
  - [ ] Create `scripts/backup_database.py`
  - [ ] Use SQLite .backup command
  - [ ] Add timestamp to backup filename
  - [ ] Store in `backups/` directory
  - [ ] Add cleanup for old backups (keep last 10)
  - [ ] Return backup file path

- [ ] Create migration validation script (AC: 2)
  - [ ] Create `scripts/validate_migration.py`
  - [ ] Count total JSON records
  - [ ] Count total database records
  - [ ] Compare counts and report discrepancies
  - [ ] Sample-check 10 random records for data integrity
  - [ ] Verify all fields present and correct types
  - [ ] Generate validation report
  - [ ] Exit with error code if validation fails

- [ ] Create end-to-end test suite (AC: 3)
  - [ ] Create `tests/test_end_to_end.py`
  - [ ] Test 1: Complete workflow (scrape → match → display)
  - [ ] Test 2: Duplicate detection (run twice, verify 0 duplicates second time)
  - [ ] Test 3: Early exit (verify stops at all-duplicate page)
  - [ ] Test 4: Multi-search (verify same job for different searches)
  - [ ] Test 5: CV change (verify re-evaluation with new cv_key)
  - [ ] Test 6: Database query performance (verify <100ms)
  - [ ] Test 7: Backward compatibility (JSON fallback works)
  - [ ] All tests use isolated test database

- [ ] Create performance benchmark suite (AC: 4)
  - [ ] Create `scripts/benchmark_performance.py`
  - [ ] Measure: Baseline (before) vs New (after)
  - [ ] Metric 1: API calls on repeat run
  - [ ] Metric 2: Pages scraped on repeat run
  - [ ] Metric 3: Job lookup query time
  - [ ] Metric 4: Total workflow execution time
  - [ ] Generate comparison report
  - [ ] Calculate cost savings (API call reduction × cost per call)
  - [ ] Save results to `benchmarks/results_YYYYMMDD.json`

- [ ] Create production deployment checklist (AC: 5)
  - [ ] Create `DEPLOYMENT.md` document
  - [ ] Pre-deployment:
    - [ ] Run all tests and verify passing
    - [ ] Create database backup
    - [ ] Review code changes
    - [ ] Schedule maintenance window
    - [ ] Notify users of deployment
  - [ ] Deployment steps:
    - [ ] Pull latest code: `git pull origin main`
    - [ ] Install dependencies: `pip install -r requirements.txt`
    - [ ] Initialize database: `python init_db.py`
    - [ ] Run migration: `python scripts/migrate_json_to_sqlite.py`
    - [ ] Validate migration: `python scripts/validate_migration.py`
    - [ ] Restart services
  - [ ] Post-deployment:
    - [ ] Run smoke tests
    - [ ] Monitor logs for errors
    - [ ] Verify performance metrics
    - [ ] Check user access
  - [ ] Rollback procedure:
    - [ ] Stop services
    - [ ] Restore database: `cp backups/jobsearchai_backup_YYYYMMDD.db instance/jobsearchai.db`
    - [ ] Revert code: `git revert <commit>`
    - [ ] Restart services
    - [ ] Verify functionality

- [ ] Update documentation (AC: 6)
  - [ ] Update `docs/USER-QUICK-START-GUIDE.md`:
    - [ ] Add section on database features
    - [ ] Document filtering capabilities
    - [ ] Explain CV versioning
    - [ ] Document search term tracking
  - [ ] Update `docs/development-guide.md`:
    - [ ] Add database architecture section
    - [ ] Document utils/db_utils.py API
    - [ ] Document utils/cv_utils.py API
    - [ ] Add troubleshooting tips
  - [ ] Create `docs/TROUBLESHOOTING-GUIDE.md`:
    - [ ] Database locked errors
    - [ ] Migration issues
    - [ ] Performance problems
    - [ ] Deduplication not working
  - [ ] Update `docs/tech-spec.md`:
    - [ ] Document database schema
    - [ ] Document API changes
    - [ ] Update architecture diagrams

- [ ] Configure monitoring and alerting (AC: 7)
  - [ ] Add database monitoring:
    - [ ] Track database file size growth
    - [ ] Monitor query performance
    - [ ] Track connection pool usage
  - [ ] Add API usage tracking:
    - [ ] Count OpenAI API calls per day
    - [ ] Track API call savings (duplicates skipped)
    - [ ] Monitor API errors
  - [ ] Add error monitoring:
    - [ ] Track IntegrityErrors (duplicates)
    - [ ] Track OperationalErrors (database locked)
    - [ ] Alert on error rate increase
  - [ ] Add deduplication metrics:
    - [ ] Track duplicate rate per scrape
    - [ ] Monitor early exit effectiveness
    - [ ] Calculate cumulative savings
  - [ ] Configure logging:
    - [ ] Ensure all components log to central location
    - [ ] Set appropriate log levels (INFO for production)
    - [ ] Rotate logs to prevent disk fill

- [ ] Perform production deployment (AC: 5)
  - [ ] Schedule maintenance window
  - [ ] Create production database backup
  - [ ] Deploy code changes
  - [ ] Run database initialization
  - [ ] Execute migration script
  - [ ] Validate migration
  - [ ] Run smoke tests
  - [ ] Monitor for issues

- [ ] Conduct production verification (AC: 8)
  - [ ] Monitor for 48 hours post-deployment
  - [ ] Track error logs daily
  - [ ] Verify performance benchmarks met
  - [ ] Check no data loss occurred
  - [ ] Run regression tests
  - [ ] Collect user feedback
  - [ ] Document any issues found
  - [ ] Create follow-up tasks if needed

---

## Dev Notes

### Migration Script Structure

```python
# scripts/migrate_json_to_sqlite.py

import json
import glob
from pathlib import Path
from datetime import datetime
from utils.db_utils import JobMatchDatabase
from utils.cv_utils import generate_cv_key

def migrate_legacy_json_files(
    json_dir="job_matches",
    db_path="instance/jobsearchai.db",
    dry_run=False
):
    """
    Migrate existing JSON match files to SQLite database
    
    Interactive migration with user prompts for missing data
    """
    db = JobMatchDatabase(db_path)
    db.connect()
    db.init_database()
    
    json_files = sorted(glob.glob(f"{json_dir}/job_matches_*.json"))
    
    if not json_files:
        print("No JSON files found to migrate")
        return
    
    print(f"Found {len(json_files)} JSON files to migrate")
    
    total_migrated = 0
    total_duplicates = 0
    total_errors = 0
    
    for json_file in json_files:
        print(f"\n{'='*60}")
        print(f"Processing: {json_file}")
        print(f"{'='*60}")
        
        with open(json_file, 'r', encoding='utf-8') as f:
            matches = json.load(f)
        
        print(f"File contains {len(matches)} matches")
        
        # Prompt for metadata
        search_term = input("Enter search term for this file (e.g., 'IT'): ").strip()
        cv_path = input("Enter CV path used (e.g., 'process_cv/cv-data/input/Lebenslauf.pdf'): ").strip()
        
        if not search_term or not cv_path:
            print("⚠️  Skipping file - missing search term or CV path")
            continue
        
        # Generate CV key
        try:
            cv_key = generate_cv_key(cv_path)
            print(f"✓ Generated CV key: {cv_key}")
        except FileNotFoundError:
            print(f"⚠️  CV file not found: {cv_path}")
            cv_key = input("Enter CV key manually (16 hex chars, or press Enter to skip): ").strip()
            if not cv_key or len(cv_key) != 16:
                print("⚠️  Invalid CV key, skipping file")
                continue
        
        if dry_run:
            print(f"[DRY RUN] Would migrate {len(matches)} matches")
            continue
        
        # Migrate each match
        for i, match in enumerate(matches, 1):
            try:
                match_data = {
                    'job_url': match.get('application_url') or match.get('Application URL', ''),
                    'search_term': search_term,
                    'cv_key': cv_key,
                    'job_title': match.get('job_title') or match.get('Job Title', ''),
                    'company_name': match.get('company_name') or match.get('Company Name', ''),
                    'location': match.get('location') or match.get('Location', ''),
                    'posting_date': match.get('posting_date'),
                    'salary_range': match.get('salary_range'),
                    'overall_match': match.get('overall_match', 0),
                    'skills_match': match.get('skills_match'),
                    'experience_match': match.get('experience_match'),
                    'education_fit': match.get('education_fit'),
                    'career_trajectory_alignment': match.get('career_trajectory_alignment'),
                    'preference_match': match.get('preference_match'),
                    'potential_satisfaction': match.get('potential_satisfaction'),
                    'location_compatibility': match.get('location_compatibility'),
                    'reasoning': match.get('reasoning', ''),
                    'scraped_data': match,  # Store complete match
                    'scraped_at': datetime.now().isoformat()
                }
                
                row_id = db.insert_job_match(match_data)
                
                if row_id:
                    total_migrated += 1
                    print(f"  [{i}/{len(matches)}] ✓ {match_data['job_title']}")
                else:
                    total_duplicates += 1
                    print(f"  [{i}/{len(matches)}] ⊘ Duplicate")
                    
            except Exception as e:
                total_errors += 1
                print(f"  [{i}/{len(matches)}] ✗ Error: {str(e)}")
    
    db.close()
    
    # Summary
    print(f"\n{'='*60}")
    print(f"Migration Complete")
    print(f"{'='*60}")
    print(f"Total migrated: {total_migrated}")
    print(f"Total duplicates: {total_duplicates}")
    print(f"Total errors: {total_errors}")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--dry-run', action='store_true', help='Preview migration without making changes')
    args = parser.parse_args()
    
    migrate_legacy_json_files(dry_run=args.dry_run)
```

### Database Backup Script

```python
# scripts/backup_database.py

import os
import sqlite3
from datetime import datetime
from pathlib import Path

def backup_database(
    db_path="instance/jobsearchai.db",
    backup_dir="backups"
):
    """Create timestamped backup of database"""
    
    # Create backup directory
    Path(backup_dir).mkdir(exist_ok=True)
    
    # Generate backup filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = f"{backup_dir}/jobsearchai_backup_{timestamp}.db"
    
    # Create backup using SQLite .backup
    try:
        src_conn = sqlite3.connect(db_path)
        dst_conn = sqlite3.connect(backup_file)
        
        src_conn.backup(dst_conn)
        
        src_conn.close()
        dst_conn.close()
        
        print(f"✓ Database backed up to: {backup_file}")
        
        # Cleanup old backups (keep last 10)
        cleanup_old_backups(backup_dir, keep=10)
        
        return backup_file
        
    except Exception as e:
        print(f"✗ Backup failed: {e}")
        return None

def cleanup_old_backups(backup_dir, keep=10):
    """Remove old backups, keeping only the most recent"""
    backups = sorted(Path(backup_dir).glob("jobsearchai_backup_*.db"))
    
    if len(backups) > keep:
        to_remove = backups[:-keep]
        for backup in to_remove:
            backup.unlink()
            print(f"  Removed old backup: {backup.name}")

if __name__ == "__main__":
    backup_database()
```

### Performance Benchmark

```python
# scripts/benchmark_performance.py

import time
import statistics
from utils.db_utils import JobMatchDatabase

def benchmark_query_performance(iterations=100):
    """Benchmark database query performance"""
    
    db = JobMatchDatabase()
    db.connect()
    
    # Populate test data if needed
    cursor = db.conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM job_matches")
    count = cursor.fetchone()[0]
    
    if count < 100:
        print("Not enough data for benchmark (need at least 100 records)")
        return
    
    # Benchmark: Single job lookup
    cursor.execute("SELECT job_url FROM job_matches LIMIT 1")
    test_url = cursor.fetchone()[0]
    
    times = []
    for _ in range(iterations):
        start = time.time()
        cursor.execute("""
            SELECT scraped_data FROM job_matches 
            WHERE job_url = ? 
            LIMIT 1
        """, (test_url,))
        cursor.fetchone()
        elapsed = time.time() - start
        times.append(elapsed * 1000)  # Convert to ms
    
    # Statistics
    avg_time = statistics.mean(times)
    median_time = statistics.median(times)
    p95_time = sorted(times)[int(len(times) * 0.95)]
    
    print(f"\n{'='*60}")
    print(f"Query Performance Benchmark ({iterations} iterations)")
    print(f"{'='*60}")
    print(f"Average: {avg_time:.2f}ms")
    print(f"Median:  {median_time:.2f}ms")
    print(f"P95:     {p95_time:.2f}ms")
    print(f"Target:  <100ms")
    print(f"Status:  {'✓ PASS' if p95_time < 100 else '✗ FAIL'}")
    print(f"{'='*60}\n")
    
    db.close()
    
    return {
        'avg': avg_time,
        'median': median_time,
        'p95': p95_time,
        'pass': p95_time < 100
    }

if __name__ == "__main__":
    benchmark_query_performance()
```

### Deployment Checklist

**File:** `DEPLOYMENT.md`

```markdown
# Production Deployment Checklist

## Pre-Deployment

- [ ] All tests passing (`pytest tests/`)
- [ ] Code review completed
- [ ] Performance benchmarks meet targets
- [ ] Backup created (`python scripts/backup_database.py`)
- [ ] Maintenance window scheduled
- [ ] Users notified

## Deployment Steps

1. **Pull Latest Code**
   ```bash
   cd /path/to/JobSearchAI
   git pull origin main
   ```

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Initialize Database**
   ```bash
   python init_db.py
   ```

4. **Run Migration**
   ```bash
   python scripts/migrate_json_to_sqlite.py
   ```

5. **Validate Migration**
   ```bash
   python scripts/validate_migration.py
   ```
   - Exit code 0 = success
   - Exit code 1 = validation failed (DO NOT PROCEED)

6. **Restart Services**
   ```bash
   # Restart web server/application
   systemctl restart jobsearchai  # or equivalent
   ```

## Post-Deployment

- [ ] Smoke tests pass
- [ ] No errors in logs (check last 100 lines)
- [ ] Database queries working (<100ms)
- [ ] Deduplication working (check scrape_history)
- [ ] Users can access system
- [ ] Monitor for 48 hours

## Rollback Procedure

If issues arise:

1. **Stop Services**
   ```bash
   systemctl stop jobsearchai
   ```

2. **Restore Database**
   ```bash
   cp backups/jobsearchai_backup_YYYYMMDD_HHMMSS.db instance/jobsearchai.db
   ```

3. **Revert Code**
   ```bash
   git revert <commit-hash>
   ```

4. **Restart Services**
   ```bash
   systemctl start jobsearchai
   ```

5. **Verify Rollback**
   - Check system is accessible
   - Verify existing functionality works
   - Debug issues in development
```

### Smoke Tests

```python
# tests/test_smoke.py

def test_database_connection():
    """Verify database is accessible"""
    from utils.db_utils import JobMatchDatabase
    db = JobMatchDatabase()
    db.connect()
    assert db.conn is not None
    db.close()

def test_scraper_imports():
    """Verify scraper can import database utilities"""
    from job_matcher import match_jobs_with_cv_dedup
    assert callable(match_jobs_with_cv_dedup)

def test_dashboard_database_query():
    """Verify dashboard can query database"""
    from dashboard import get_job_details_for_url
    # Will return None or fallback to JSON if no data
    result = get_job_details_for_url("test_url")
    assert result is not None or result is None  # Either result is valid

def test_deduplication_check():
    """Verify deduplication check works"""
    from utils.db_utils import JobMatchDatabase
    db = JobMatchDatabase()
    db.connect()
    db.init_database()
    
    exists = db.job_exists("test_url", "test_search", "test_cv")
    assert isinstance(exists, bool)
    
    db.close()
```

### Monitoring Configuration

Add to logging configuration:

```python
# In config.py or logging setup

MONITORING_CONFIG = {
    'database': {
        'max_size_mb': 1000,  # Alert if DB exceeds 1GB
        'slow_query_threshold_ms': 100,
        'check_interval_seconds': 300
    },
    'api': {
        'track_calls': True,
        'alert_on_increase': True,
        'expected_reduction': 0.9  # 90% reduction
    },
    'errors': {
        'alert_threshold': 10,  # Alert if >10 errors in 1 hour
        'critical_errors': ['OperationalError', 'DatabaseError']
    }
}
```

### Expected Outcomes

**Pre-Migration State:**
- JSON files in `job_matches/` directory
- Multiple files per scraping session
- No deduplication
- Slow queries

**Post-Migration State:**
- All data in `instance/jobsearchai.db`
- JSON files archived/backed up
- Deduplication active
- Fast queries (<100ms)
- Monitoring enabled

**Success Metrics:**
- Zero data loss during migration
- 90% reduction in API calls (repeat runs)
- 70% reduction in page scraping (repeat runs)
- <100ms query response time
- No critical bugs in first week
- Positive user feedback

### Rollback Triggers

Initiate rollback if:
- Data loss detected
- Critical bugs affecting core functionality
- Performance degradation >50%
- Error rate >5%
- Database corruption
- User-reported blocking issues

### Architecture Reference

For complete technical details, see:
- [UNIFIED-ARCHITECTURE-DOCUMENT.md](../System%20A%20and%20B%20Improvement%20Plan/UNIFIED-ARCHITECTURE-DOCUMENT.md) - Section "Phase 4: Migration & Testing" and "Phase 5: Production Deployment"

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-02 | 1.0 | Story created from architecture Phases 4 & 5 | PM (John) |

---

## Dev Agent Record

### Agent Model Used
_To be filled by dev agent_

### Debug Log References
_To be filled by dev agent_

### Completion Notes List
_To be filled by dev agent_

### File List
_To be filled by dev agent_

---

## QA Results
_To be filled by QA agent_
